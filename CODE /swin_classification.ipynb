{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b92774-6b88-451b-9d5d-64179ae7aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17509c9c-cad3-439a-bf7b-82f4f976adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56061a-3118-4314-907c-f160bf888192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5177cb-cead-41be-9f55-52baa4b413cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import zipfile\n",
    "import pandas as pd \n",
    "import torch.optim as optim\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17a580-376a-471c-94f4-c4bba5f693ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420bb6c-7b6f-416e-98d7-1521e1bfcc5d",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d5b4e-ca3f-494e-b2db-af02bdc4838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(timm.list_models(\"swin*\", pretrained = True))\n",
    "\n",
    "model = timm.create_model('swin_base_patch4_window7_224', pretrained = True)\n",
    "\n",
    "in_features = model.head.in_features\n",
    "model.head = nn.Linear(in_features, 2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(model.head)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8277507-6cec-4095-8ef8-64123b412ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name == 'head.weight' or name == 'head.bias':\n",
    "        print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19888d77-6892-425a-9bdf-fb75588df010",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f64afa-fcdd-4c7f-9a78-8c3dde899b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_file = zipfile.ZipFile('/scratch/mmpate15/pe_classification/data/train/train.zip')\n",
    "csv_file = zip_file.open('train.csv')\n",
    "\n",
    "df = pd.read_csv(csv_file, index_col =False)\n",
    "zip_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622946a3-108b-4eda-97eb-71882e499661",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folder_names = sorted( os.listdir('/scratch/mmpate15/pe_classification/data/train/train'))\n",
    "\n",
    "print(len(all_folder_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96b2cd-bf14-4a24-ad5b-10a50e1f2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9cc7a-f6c3-46f3-a1e8-3bf8792e20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_study_ids, test_study_ids = train_test_split(all_folder_names, train_size=100, test_size=100, shuffle=True, random_state=42)\n",
    "\n",
    "# Print the number of folders in each set\n",
    "print(f'Number of folders in training set: {len(train_study_ids)}')\n",
    "print(f'Number of folders in testing set: {len(test_study_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb1d14-a25b-4c01-a1ac-9bb000e88c86",
   "metadata": {},
   "source": [
    "## Training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ee3a1-f904-4f71-ac75-a1739ff24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows that match the specified StudyID values for training data\n",
    "mask = df['StudyInstanceUID'].isin(train_study_ids)\n",
    "train_filtered_df = df[mask]\n",
    "train_filtered_df = train_filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d822b-c914-48e6-8435-a26000e77f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = train_filtered_df.iloc[:, :4]\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8118afb-a6d4-4b1f-9126-718a9cc3033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = X_df[\"pe_present_on_image\"].value_counts()\n",
    "print('inital class counts: \\n', class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed954c-ff3e-4e93-8121-aa7115070b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = int(class_counts[1] + (class_counts[1]*0.2))\n",
    "max_count_0 = min(numb, class_counts[0]) # Set a limit of 500 for class 0\n",
    "max_count_1 = class_counts[1]  \n",
    "\n",
    "train_df_filtered = pd.concat([X_df[X_df[\"pe_present_on_image\"]==0][:max_count_0], X_df[X_df[\"pe_present_on_image\"]==1][:max_count_1]])\n",
    "train_df_filtered = train_df_filtered.reset_index(drop=True)\n",
    "train_df_filtered = train_df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "(train_df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233c563-e69b-4ad0-8b86-794ac0d3da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## balanced dataframe class count\n",
    "\n",
    "class_counts = train_df_filtered[\"pe_present_on_image\"].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb483de-cef2-4611-8f01-d3c626092870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6058d0a8-0a5d-427e-ac0d-5c9e7190398b",
   "metadata": {},
   "source": [
    "## Testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b531b83-f3d6-4d27-b03f-ce42df9858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows that match the specified StudyID values for testing data\n",
    "test_mask = df['StudyInstanceUID'].isin(test_study_ids)\n",
    "test_filtered_df = df[test_mask]\n",
    "test_filtered_df = test_filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8effb-2d4f-49c8-b089-f75043f1067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = test_filtered_df.iloc[:, :4]\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c606c5-4a08-459d-a11f-4377811c1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_df[\"pe_present_on_image\"].value_counts()\n",
    "print('inital class counts: \\n', class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39755e5-3d02-4659-a649-5445fb4acc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "numb =0\n",
    "class_count = 0\n",
    "max_count_0 = 0\n",
    "max_count_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75644c3c-f481-4624-81be-6c8e8f524d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = int(class_counts[1] + (class_counts[1]*0.2))\n",
    "max_count_0 = min(numb, class_counts[0]) # Set a limit of 500 for class 0\n",
    "max_count_1 = class_counts[1]  \n",
    "\n",
    "test_df_filtered = pd.concat([y_df[y_df[\"pe_present_on_image\"]==0][:max_count_0], y_df[y_df[\"pe_present_on_image\"]==1][:max_count_1]])\n",
    "# test_df_filtered = test_df_filtered.reset_index(drop=True)\n",
    "test_df_filtered = test_df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "(test_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2a6b9-4ead-4284-9f75-c84bc4aa5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = test_df_filtered[\"pe_present_on_image\"].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429a9e5-8b41-4687-a8ab-8d450f15b398",
   "metadata": {},
   "source": [
    "## Creating Custom DataSet and Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950c86e-8446-4e4c-b887-1fdefa29b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scratch/mmpate15/pe_classification/data/train/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180408b1-8d75-4f4f-bc14-52bc8892f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = X_df\n",
    "\n",
    "# img_path = os.path.join(root_dir, data.iloc[1]['StudyInstanceUID'], data.iloc[1]['SeriesInstanceUID'],\n",
    "#                                 data.iloc[1]['SOPInstanceUID'] + '.dcm').replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27837f-193b-48b5-9ecd-0bd7bac5feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    classes = [0, 1]\n",
    "\n",
    "    def __init__(self, root_dir, df, transform):\n",
    "        self.data = df\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['StudyInstanceUID'], self.data.iloc[idx]['SeriesInstanceUID'],\n",
    "                                self.data.iloc[idx]['SOPInstanceUID'] + '.dcm').replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        \n",
    "        dcm = pydicom.read_file(img_path).pixel_array\n",
    "        img = Image.fromarray(np.uint8(dcm * 255), 'L')\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # img = transforms.ToTensor()(img)\n",
    "\n",
    "\n",
    "\n",
    "        label = torch.tensor(int(self.data['pe_present_on_image'][idx]))\n",
    "        \n",
    "        return img, label, img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc947445-aca6-448e-88ac-c9c10024385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Resize(224),\n",
    "                   T.ToTensor(),\n",
    "                   T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4c15a-41c8-468f-a2ee-b974c810b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading the DataSet\n",
    "\n",
    "train_dataset = MyDataset(root_dir, train_df_filtered, transform)\n",
    "test_dataset = MyDataset(root_dir, test_df_filtered, transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9de6e3-791a-4696-83a5-0bab90756ae6",
   "metadata": {},
   "source": [
    "## Visualizing the Data Set and class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde04786-bd39-4cf3-944a-edbc6d9ab190",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0] * len(MyDataset.classes)  # initialize count to 0 for each class\n",
    "\n",
    "for images, labels,_ in train_loader:\n",
    "    images = images.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    # print(labels)\n",
    "    for i in range(len(unique_labels)):\n",
    "        label = int(unique_labels[i])\n",
    "        count[label] += counts[i]\n",
    "\n",
    "\n",
    "\n",
    "print(images.size())\n",
    "print('original labels:', labels)\n",
    "\n",
    "# Visualize the class distribution using a bar plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(MyDataset.classes, count)\n",
    "ax.set_xlabel('Class label')\n",
    "ax.set_ylabel('Number of instances')\n",
    "ax.set_title('Class Distribution')\n",
    "for i, v in enumerate(count):\n",
    "    ax.text(i, v+0, str(v), color='blue', ha='center')\n",
    "plt.xticks([0,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa49824-6fa4-491f-b97f-fc0e7dd8361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c9bbc-8299-4895-a21b-310c939be185",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0] * len(MyDataset.classes)  # initialize count to 0 for each class\n",
    "\n",
    "for images, labels,_ in test_loader:\n",
    "    images = images.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    # print(labels)\n",
    "    for i in range(len(unique_labels)):\n",
    "        label = int(unique_labels[i])\n",
    "        count[label] += counts[i]\n",
    "\n",
    "\n",
    "\n",
    "print(images.size())\n",
    "print('original labels:', labels)\n",
    "\n",
    "# Visualize the class distribution using a bar plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(MyDataset.classes, count)\n",
    "ax.set_xlabel('Class label')\n",
    "ax.set_ylabel('Number of instances')\n",
    "ax.set_title('Class Distribution')\n",
    "for i, v in enumerate(count):\n",
    "    ax.text(i, v+0, str(v), color='blue', ha='center')\n",
    "plt.xticks([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408961e6-0af7-4e10-9014-fad2f21ed9e4",
   "metadata": {},
   "source": [
    "## Creating the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f058fd-5178-4fe1-9f30-0c5036902bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "# Define the class weights\n",
    "# class_weights = torch.tensor([1.0, 30.0])\n",
    "# class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "# weight=class_weights\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80943965-a596-4d47-893a-ff94c6941321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42f73e-4c06-4957-8c4f-60cd68e47c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the training loop\n",
    "\n",
    "\n",
    "f1_list = []\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    num_samples = 0.0    \n",
    "    conf_matrix = [[0, 0], [0, 0]]\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels, _ = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # outputs = outputs.to(device)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "         # Update statistics\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        num_samples += inputs.size(0)\n",
    "        \n",
    "        # Update the confusion matrix\n",
    "        conf_matrix += confusion_matrix(labels.cpu(), preds.cpu(), labels=[0, 1])\n",
    "\n",
    "        # print('\\n[%d, %5d] loss: %.3f, accuracy: %.3f' % (epoch + 1, i + 1, running_loss / num_samples, running_corrects / num_samples))\n",
    "        \n",
    "        # Collect predictions and true labels for f1 score calculation\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset)\n",
    "    train_loss_list.append(epoch_loss)\n",
    "    train_acc_list.append(epoch_acc)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    print('\\nTrain Set: Epoch [%d/%d], Loss: %.4f, Accuracy: %.4f, F1: %.4f' % (epoch+1, num_epochs, epoch_loss, epoch_acc, f1))\n",
    "\n",
    "print('Finished Training & saved the model')\n",
    "\n",
    "print(\"\\n Here is the testing confusion matrix: \\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f90126-f186-4c4b-9f17-166f3b72af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights after training\n",
    "torch.save(model.state_dict(), 'swin_transformer_classification.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78f03f-5e02-47ba-af96-e8c8563a63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "epochs = list(range(1, 11))\n",
    "loss = [0.5457, 0.3868, 0.3157, 0.2729, 0.2442, 0.2178, 0.1992, 0.1808, 0.1732, 0.1651]\n",
    "accuracy = [0.7500, 0.8801, 0.9107, 0.9318, 0.9291, 0.9463, 0.9487, 0.9553, 0.9522, 0.9604]\n",
    "f1_score = [0.7489, 0.8802, 0.9107, 0.9319, 0.9291, 0.9463, 0.9487, 0.9553, 0.9522, 0.9604]\n",
    "\n",
    "# Loss plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, loss, '-o', linewidth=2)\n",
    "plt.title('Training Loss', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, accuracy, '-o', linewidth=2)\n",
    "plt.title('Training Accuracy', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim([0, 1.0])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# F1 score plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, f1_score, '-o', linewidth=2)\n",
    "plt.title('Training F1 Score', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.ylim([0, 1.0])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae1f0a-3afd-4bae-a440-bb1fbd5cf475",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Testing the model\n",
    "\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "results = []\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0.0\n",
    "    total = 0.0    \n",
    "    conf_matrix = [[0, 0], [0, 0]]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, img_names) in enumerate(test_loader):\n",
    "            # Forward pass\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update loss\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Update accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "                        \n",
    "            \n",
    "            for i in range(len(predicted)):\n",
    "                print(\"Image: {}, Prediction: {},\".format(img_names[i],predicted[i].item()))\n",
    "\n",
    "                results.append((img_names[i], predicted[i].item()))\n",
    "                \n",
    "            # Update the confusion matrix\n",
    "            conf_matrix += confusion_matrix(targets.cpu(), predicted.cpu(), labels=[0, 1])\n",
    "\n",
    "\n",
    "            total += targets.size(0)\n",
    "            test_correct += (predicted == targets).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = test_correct / len(test_dataset)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "    \n",
    "print(\"\\nFinished Testing the model\")\n",
    "    \n",
    "print(\"\\n Here is the testing confusion matrix: \\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda40a9-3dea-4821-a152-a7da064c97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/scratch/mmpate15/pe_classification/swin_transformer_classification.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
