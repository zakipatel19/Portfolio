{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f71b02-92e8-486b-89b2-07c686dd438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f54670-c476-4931-8734-ba5fbe3a50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count() \n",
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0338dc-baca-468d-94ab-6724e7449102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from unet import UNet\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.io as io\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import whosmat\n",
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms as T\n",
    "import timm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1d6b6-9ffc-4794-bbcf-110654a43993",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Resize(224),\n",
    "                   T.ToTensor(),\n",
    "                   T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851fbd7-bb19-423d-90a6-a137a21b31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.5], [0.3])\n",
    "    ])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.1], [0.1])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a5ff9-6f84-4c39-a5e7-eb03655c8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, scans_path, ground_truth_path, transform=transform, background_ratio=0.3):\n",
    "        self.scans_path = scans_path\n",
    "        self.ground_truth_path = ground_truth_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "        self.background_samples = []\n",
    "        self.object_samples = []\n",
    "\n",
    "        # Get a list of all patient folders\n",
    "        patients = sorted(os.listdir(scans_path))\n",
    "\n",
    "        for patient_folder in patients:\n",
    "            patient_path = os.path.join(scans_path, patient_folder)\n",
    "            ground_truth_file = os.path.join(ground_truth_path, f\"{patient_folder}.mat\")\n",
    "\n",
    "            # Load ground truth masks from .mat file\n",
    "            ground_truth_data = loadmat(ground_truth_file)\n",
    "            masks = ground_truth_data[\"Mask\"]\n",
    "\n",
    "            # Get list of .dcm files for the patient\n",
    "            dcm_files = sorted([f for f in os.listdir(patient_path) if f.endswith(\".dcm\")])\n",
    "\n",
    "            for i, dcm_file in enumerate(dcm_files):\n",
    "                dcm_path = os.path.join(patient_path, dcm_file)\n",
    "                mask = masks[:, :, i]\n",
    "\n",
    "                if np.sum(mask) == 0:\n",
    "                    self.background_samples.append((dcm_path, mask))\n",
    "                else:\n",
    "                    self.object_samples.append((dcm_path, mask))\n",
    "\n",
    "        # Calculate the number of background samples to keep\n",
    "        num_background_samples = int(len(self.background_samples))\n",
    "\n",
    "        # Shuffle and truncate the background_samples list\n",
    "        random.shuffle(self.background_samples)\n",
    "        self.background_samples = self.background_samples[:num_background_samples]\n",
    "\n",
    "        # Combine object and background samples into the final list\n",
    "        self.samples = self.background_samples + self.object_samples\n",
    "        random.shuffle(self.samples)\n",
    "        \n",
    "        print(\"list object:\",len(self.object_samples))\n",
    "        print(\"list background:\",len(self.background_samples))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dcm_path, mask = self.samples[idx]\n",
    "\n",
    "        # Read .dcm file\n",
    "        dcm = pydicom.read_file(dcm_path).pixel_array\n",
    "        img = Image.fromarray(np.uint8(dcm * 255), 'L')\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "        mask = Image.fromarray(np.uint8(mask), 'L')\n",
    "        # mask = mask.convert('RGB')\n",
    "\n",
    "        # Apply transformation, if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35a3ee-c722-4884-b7ca-da89a0619412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your custom train dataset\n",
    "train_scans_path = \"data/train_scans\"\n",
    "train_ground_truth_path = \"data/train_truth\"\n",
    "transform = ToTensor()\n",
    "train_dataset = CTDataset(train_scans_path, train_ground_truth_path, transform=transform)\n",
    "\n",
    "# Create a DataLoader to handle batching\n",
    "batch_size = 12\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eacd1d-fe46-4e6a-aaef-143ecfa2f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = train_dataset[100]\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea55e70-f391-4b74-b641-1b88c44d43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image100, mask100 = train_dataset[18]\n",
    "image60, mask60 = train_dataset[60]\n",
    "image17, mask17 = train_dataset[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24853963-b767-4158-a3c8-f99a40417650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image_pairs):\n",
    "    \"\"\"Plot image-mask pairs vertically below each other.\"\"\"\n",
    "    num_pairs = len(image_pairs)\n",
    "    plt.figure(figsize=(10, 5 * num_pairs))\n",
    "    for i, (image, mask) in enumerate(image_pairs):\n",
    "        plt.subplot(num_pairs, 2, i * 2 + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        plt.subplot(num_pairs, 2, i * 2 + 2)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"Mask {i+1}\")\n",
    "        plt.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a7fbe-5498-4b81-88ff-ecb75deaa5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image100 = np.transpose(image100,(1,2,0))\n",
    "mask100 = np.transpose(mask100, (1,2,0))\n",
    "image55 = np.transpose(image60,(1,2,0))\n",
    "mask55 = np.transpose(mask60, (1,2,0))\n",
    "image17 = np.transpose(image17,(1,2,0))\n",
    "mask17 = np.transpose(mask17, (1,2,0))\n",
    "\n",
    "\n",
    "image_pairs = [(image100, mask100), (image55,mask55),(image17,mask17)]\n",
    "visualize(image_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13f12f-788a-41d3-ab5f-5939360c92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e2a45-9c56-402e-bf36-8ac1b1c4204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3683c46-664d-4230-9540-6f35cb2b0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b791a-c22a-4baa-a2c2-f245b4190d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper module that consists of a Conv -> BN -> ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.with_nonlinearity = with_nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.with_nonlinearity:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bridge(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the middle layer of the UNet which just consists of some\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bridge = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bridge(x)\n",
    "\n",
    "\n",
    "class UpBlockForUNetWithResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
    "                 upsampling_method=\"conv_transpose\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if up_conv_in_channels == None:\n",
    "            up_conv_in_channels = in_channels\n",
    "        if up_conv_out_channels == None:\n",
    "            up_conv_out_channels = out_channels\n",
    "\n",
    "        if upsampling_method == \"conv_transpose\":\n",
    "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
    "        elif upsampling_method == \"bilinear\":\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
    "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, up_x, down_x):\n",
    "        \"\"\"\n",
    "        :param up_x: this is the output from the previous up block\n",
    "        :param down_x: this is the output from the down block\n",
    "        :return: upsampled feature map\n",
    "        \"\"\"\n",
    "        x = self.upsample(up_x)\n",
    "        x = torch.cat([x, down_x], 1)\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self, n_classes=1):\n",
    "        super().__init__()\n",
    "        # ResNet50_Weights.IMAGENET1K_V1\n",
    "        resnet = torchvision.models.resnet.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "        self.input_pool = list(resnet.children())[3]\n",
    "        for bottleneck in list(resnet.children()):\n",
    "            if isinstance(bottleneck, nn.Sequential):\n",
    "                down_blocks.append(bottleneck)\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n",
    "\n",
    "                                                \n",
    "    \n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i == (UNet.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNet.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16483f1-88ba-4e38-ad87-1e6ee16b348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "\n",
    "model= nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100123db-2fa7-40cc-8ce5-fe22f8570b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight = torch.tensor([6]).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94115c-f783-4473-b41f-eac1c98520d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2.0 * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "def iou(pred, target, n_classes=1):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    pred_inds = pred > 0.5\n",
    "    target_inds = target > 0.5\n",
    "    intersection = (pred_inds & target_inds).float().sum().item()\n",
    "    union = (pred_inds | target_inds).float().sum().item()\n",
    "    iou_score = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return iou_score\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_iou = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # labels = labels.squeeze(1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print(inputs.shape)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        dice_score = dice_coefficient(labels, outputs)\n",
    "        iou_score = iou(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_score\n",
    "        running_iou += iou_score\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_dice = running_dice / len(train_loader)\n",
    "    epoch_iou = running_iou / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.3f}, Dice Score: {epoch_dice:.3f}, IoU Score: {epoch_iou:.3f}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet50_unet_pretrained2.pth\")\n",
    "print(\"\\nModel saved to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57cf0e-049e-4e50-b22f-58f774ae6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgt,maskt = train_dataset[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34647a6b-40c5-4c0b-8e87-91e266ad8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgp = imgt\n",
    "\n",
    "maskt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7e4ca-454e-49b6-8d19-a15979ad963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "batch_size = 5\n",
    "imgt = imgt.unsqueeze(0)\n",
    "print(imgt.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(imgt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a819f-92e3-4f56-af1f-ea53f59ada7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((output.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb428c-c535-4745-b304-fc14bc91cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283ed21-b9ab-4299-a36f-9b3a296018fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask = (output.squeeze()>0.5).float().cpu().numpy()\n",
    "predicted_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df41df5-865d-4a80-aeaa-f3b32117ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab40963-017b-4b75-90f6-f0fbe841c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(imgt.permute(1, 2, 0).cpu().numpy())  # Transpose dimensions and convert to numpy array\n",
    "axs[0].set_title('Input Image')\n",
    "\n",
    "print(maskt.shape)\n",
    "axs[1].imshow(maskt.permute(1, 2, 0).cpu().numpy())\n",
    "axs[1].set_title('Mask')\n",
    "\n",
    "\n",
    "# print(predicted_mask.shape)\n",
    "\n",
    "axs[2].imshow(maskt.permute(1, 2, 0).cpu().numpy())\n",
    "axs[2].set_title('Predicted Mask')\n",
    "\n",
    "# Remove axis ticks\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
